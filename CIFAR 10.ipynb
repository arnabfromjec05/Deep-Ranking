{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR 10.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Bwkmvqy4hJ6p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"9747017b-7287-4a7c-80af-79e347ac5c3d","executionInfo":{"status":"ok","timestamp":1538400985718,"user_tz":-330,"elapsed":112528,"user":{"displayName":"Arnab Jana","photoUrl":"","userId":"11280183051260142922"}}},"cell_type":"code","source":["import keras\n","import numpy as np\n","from keras.datasets import cifar10\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","import os\n","\n","batch_size = 32\n","num_classes = 10\n","epochs = 2\n","data_augmentation = True\n","num_predictions = 20\n","save_dir = os.path.join(os.getcwd(), 'saved_models')\n","model_name = 'keras_cifar10_trained_model.h5'\n","\n","# The data, split between train and test sets:\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# Convert class vectors to binary class matrices.\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same',\n","                 input_shape=x_train.shape[1:]))\n","model.add(Activation('relu'))\n","model.add(Conv2D(32, (3, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(64, (3, 3), padding='same'))\n","model.add(Activation('relu'))\n","model.add(Conv2D(64, (3, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","if not data_augmentation:\n","    print('Not using data augmentation.')\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              validation_data=(x_test, y_test),\n","              shuffle=True)\n","else:\n","    print('Using real-time data augmentation.')\n","    # This will do preprocessing and realtime data augmentation:\n","    datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n","        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n","        # randomly shift images horizontally (fraction of total width)\n","        width_shift_range=0.1,\n","        # randomly shift images vertically (fraction of total height)\n","        height_shift_range=0.1,\n","        shear_range=0.,  # set range for random shear\n","        zoom_range=0.,  # set range for random zoom\n","        channel_shift_range=0.,  # set range for random channel shifts\n","        # set mode for filling points outside the input boundaries\n","        fill_mode='nearest',\n","        cval=0.,  # value used for fill_mode = \"constant\"\n","        horizontal_flip=True,  # randomly flip images\n","        vertical_flip=False,  # randomly flip images\n","        # set rescaling factor (applied before any other transformation)\n","        rescale=None,\n","        # set function that will be applied on each input\n","        preprocessing_function=None,\n","        # image data format, either \"channels_first\" or \"channels_last\"\n","        data_format=None,\n","        # fraction of images reserved for validation (strictly between 0 and 1)\n","        validation_split=0.0)\n","\n","    # Compute quantities required for feature-wise normalization\n","    # (std, mean, and principal components if ZCA whitening is applied).\n","    datagen.fit(x_train)\n","\n","    # Fit the model on the batches generated by datagen.flow().\n","    model.fit_generator(datagen.flow(x_train, y_train,\n","                                     batch_size=batch_size),\n","                        epochs=epochs,\n","                        validation_data=(x_test, y_test),\n","                        workers=4)\n","\n","# Save model and weights\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","model_path = os.path.join(save_dir, model_name)\n","model.save(model_path)\n","print('Saved trained model at %s ' % model_path)\n","\n","# Score trained model.\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["('x_train shape:', (50000, 32, 32, 3))\n","(50000, 'train samples')\n","(10000, 'test samples')\n","Using real-time data augmentation.\n","Epoch 1/2\n","1563/1563 [==============================] - 54s 35ms/step - loss: 1.8565 - acc: 0.3178 - val_loss: 1.5487 - val_acc: 0.4315\n","Epoch 2/2\n","1563/1563 [==============================] - 54s 34ms/step - loss: 1.5635 - acc: 0.4277 - val_loss: 1.3758 - val_acc: 0.5046\n","Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n","10000/10000 [==============================] - 2s 189us/step\n","('Test loss:', 1.3757537776947022)\n","('Test accuracy:', 0.5046)\n"],"name":"stdout"}]},{"metadata":{"id":"vQztpv55hr78","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"a39906ff-e2bb-47e2-ce3c-6ac24ac29b05","executionInfo":{"status":"ok","timestamp":1538397425075,"user_tz":-330,"elapsed":9039,"user":{"displayName":"Arnab Jana","photoUrl":"","userId":"11280183051260142922"}}},"cell_type":"code","source":["from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 11870088127910615707, name: \"/device:GPU:0\"\n"," device_type: \"GPU\"\n"," memory_limit: 10862300365\n"," locality {\n","   bus_id: 1\n","   links {\n","   }\n"," }\n"," incarnation: 16872828011009758713\n"," physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"AJz60-7i4A3f","colab_type":"text"},"cell_type":"markdown","source":["**TESTING FOR CLASS OF X as test input**"]},{"metadata":{"id":"xnaNLC-4sUaX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c8dabca1-c066-46a7-95e6-76d4ba287f72","executionInfo":{"status":"ok","timestamp":1538401574304,"user_tz":-330,"elapsed":926,"user":{"displayName":"Arnab Jana","photoUrl":"","userId":"11280183051260142922"}}},"cell_type":"code","source":["import numpy as np\n","x=[[[[112,100,88]]*32]*32]\n","ar=np.array(x)\n","values=model.predict_classes(ar, batch_size=None, verbose=0, steps=None)\n","print(values)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["[3]\n"],"name":"stdout"}]},{"metadata":{"id":"jiXYoBrT3ezF","colab_type":"text"},"cell_type":"markdown","source":["**CHECKING INPUT DIMENSIONS**"]},{"metadata":{"id":"sgV6snZToAXy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":5287},"outputId":"a8a7ef84-6f87-4f51-dbdf-d2f826a6b1f8","executionInfo":{"status":"ok","timestamp":1538398889355,"user_tz":-330,"elapsed":951,"user":{"displayName":"Arnab Jana","photoUrl":"","userId":"11280183051260142922"}}},"cell_type":"code","source":["\n","\n","import keras\n","from keras.datasets import cifar10\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","import os\n","\n","batch_size = 32\n","num_classes = 10\n","epochs = 2\n","data_augmentation = True\n","num_predictions = 20\n","save_dir = os.path.join(os.getcwd(), 'saved_models')\n","model_name = 'keras_cifar10_trained_model.h5'\n","\n","# The data, split between train and test sets:\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","print(x_train)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["('x_train shape:', (50000, 32, 32, 3))\n","(50000, 'train samples')\n","(10000, 'test samples')\n","[[[[ 59  62  63]\n","   [ 43  46  45]\n","   [ 50  48  43]\n","   ...\n","   [158 132 108]\n","   [152 125 102]\n","   [148 124 103]]\n","\n","  [[ 16  20  20]\n","   [  0   0   0]\n","   [ 18   8   0]\n","   ...\n","   [123  88  55]\n","   [119  83  50]\n","   [122  87  57]]\n","\n","  [[ 25  24  21]\n","   [ 16   7   0]\n","   [ 49  27   8]\n","   ...\n","   [118  84  50]\n","   [120  84  50]\n","   [109  73  42]]\n","\n","  ...\n","\n","  [[208 170  96]\n","   [201 153  34]\n","   [198 161  26]\n","   ...\n","   [160 133  70]\n","   [ 56  31   7]\n","   [ 53  34  20]]\n","\n","  [[180 139  96]\n","   [173 123  42]\n","   [186 144  30]\n","   ...\n","   [184 148  94]\n","   [ 97  62  34]\n","   [ 83  53  34]]\n","\n","  [[177 144 116]\n","   [168 129  94]\n","   [179 142  87]\n","   ...\n","   [216 184 140]\n","   [151 118  84]\n","   [123  92  72]]]\n","\n","\n"," [[[154 177 187]\n","   [126 137 136]\n","   [105 104  95]\n","   ...\n","   [ 91  95  71]\n","   [ 87  90  71]\n","   [ 79  81  70]]\n","\n","  [[140 160 169]\n","   [145 153 154]\n","   [125 125 118]\n","   ...\n","   [ 96  99  78]\n","   [ 77  80  62]\n","   [ 71  73  61]]\n","\n","  [[140 155 164]\n","   [139 146 149]\n","   [115 115 112]\n","   ...\n","   [ 79  82  64]\n","   [ 68  70  55]\n","   [ 67  69  55]]\n","\n","  ...\n","\n","  [[175 167 166]\n","   [156 154 160]\n","   [154 160 170]\n","   ...\n","   [ 42  34  36]\n","   [ 61  53  57]\n","   [ 93  83  91]]\n","\n","  [[165 154 128]\n","   [156 152 130]\n","   [159 161 142]\n","   ...\n","   [103  93  96]\n","   [123 114 120]\n","   [131 121 131]]\n","\n","  [[163 148 120]\n","   [158 148 122]\n","   [163 156 133]\n","   ...\n","   [143 133 139]\n","   [143 134 142]\n","   [143 133 144]]]\n","\n","\n"," [[[255 255 255]\n","   [253 253 253]\n","   [253 253 253]\n","   ...\n","   [253 253 253]\n","   [253 253 253]\n","   [253 253 253]]\n","\n","  [[255 255 255]\n","   [255 255 255]\n","   [255 255 255]\n","   ...\n","   [255 255 255]\n","   [255 255 255]\n","   [255 255 255]]\n","\n","  [[255 255 255]\n","   [254 254 254]\n","   [254 254 254]\n","   ...\n","   [254 254 254]\n","   [254 254 254]\n","   [254 254 254]]\n","\n","  ...\n","\n","  [[113 120 112]\n","   [111 118 111]\n","   [105 112 106]\n","   ...\n","   [ 72  81  80]\n","   [ 72  80  79]\n","   [ 72  80  79]]\n","\n","  [[111 118 110]\n","   [104 111 104]\n","   [ 99 106  98]\n","   ...\n","   [ 68  75  73]\n","   [ 70  76  75]\n","   [ 78  84  82]]\n","\n","  [[106 113 105]\n","   [ 99 106  98]\n","   [ 95 102  94]\n","   ...\n","   [ 78  85  83]\n","   [ 79  85  83]\n","   [ 80  86  84]]]\n","\n","\n"," ...\n","\n","\n"," [[[ 35 178 235]\n","   [ 40 176 239]\n","   [ 42 176 241]\n","   ...\n","   [ 99 177 219]\n","   [ 79 147 197]\n","   [ 89 148 189]]\n","\n","  [[ 57 182 234]\n","   [ 44 184 250]\n","   [ 50 183 240]\n","   ...\n","   [156 182 200]\n","   [141 177 206]\n","   [116 149 175]]\n","\n","  [[ 98 197 237]\n","   [ 64 189 252]\n","   [ 69 192 245]\n","   ...\n","   [188 195 206]\n","   [119 135 147]\n","   [ 61  79  90]]\n","\n","  ...\n","\n","  [[ 73  79  77]\n","   [ 53  63  68]\n","   [ 54  68  80]\n","   ...\n","   [ 17  40  64]\n","   [ 21  36  51]\n","   [ 33  48  49]]\n","\n","  [[ 61  68  75]\n","   [ 55  70  86]\n","   [ 57  79 103]\n","   ...\n","   [ 24  48  72]\n","   [ 17  35  53]\n","   [  7  23  32]]\n","\n","  [[ 44  56  73]\n","   [ 46  66  88]\n","   [ 49  77 105]\n","   ...\n","   [ 27  52  77]\n","   [ 21  43  66]\n","   [ 12  31  50]]]\n","\n","\n"," [[[189 211 240]\n","   [186 208 236]\n","   [185 207 235]\n","   ...\n","   [175 195 224]\n","   [172 194 222]\n","   [169 194 220]]\n","\n","  [[194 210 239]\n","   [191 207 236]\n","   [190 206 235]\n","   ...\n","   [173 192 220]\n","   [171 191 218]\n","   [167 190 216]]\n","\n","  [[208 219 244]\n","   [205 216 240]\n","   [204 215 239]\n","   ...\n","   [175 191 217]\n","   [172 190 216]\n","   [169 191 215]]\n","\n","  ...\n","\n","  [[207 199 181]\n","   [203 195 175]\n","   [203 196 173]\n","   ...\n","   [135 132 127]\n","   [162 158 150]\n","   [168 163 151]]\n","\n","  [[198 190 170]\n","   [189 181 159]\n","   [180 172 147]\n","   ...\n","   [178 171 160]\n","   [175 169 156]\n","   [175 169 154]]\n","\n","  [[198 189 173]\n","   [189 181 162]\n","   [178 170 149]\n","   ...\n","   [195 184 169]\n","   [196 189 171]\n","   [195 190 171]]]\n","\n","\n"," [[[229 229 239]\n","   [236 237 247]\n","   [234 236 247]\n","   ...\n","   [217 219 233]\n","   [221 223 234]\n","   [222 223 233]]\n","\n","  [[222 221 229]\n","   [239 239 249]\n","   [233 234 246]\n","   ...\n","   [223 223 236]\n","   [227 228 238]\n","   [210 211 220]]\n","\n","  [[213 206 211]\n","   [234 232 239]\n","   [231 233 244]\n","   ...\n","   [220 220 232]\n","   [220 219 232]\n","   [202 203 215]]\n","\n","  ...\n","\n","  [[150 143 135]\n","   [140 135 127]\n","   [132 127 120]\n","   ...\n","   [224 222 218]\n","   [230 228 225]\n","   [241 241 238]]\n","\n","  [[137 132 126]\n","   [130 127 120]\n","   [125 121 115]\n","   ...\n","   [181 180 178]\n","   [202 201 198]\n","   [212 211 207]]\n","\n","  [[122 119 114]\n","   [118 116 110]\n","   [120 116 111]\n","   ...\n","   [179 177 173]\n","   [164 164 162]\n","   [163 163 161]]]]\n"],"name":"stdout"}]},{"metadata":{"id":"NjuK4nABx42g","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}